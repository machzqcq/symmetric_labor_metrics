{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FOLDER_NAME = \"LCAProgramsH1BH1B1E3\"\n",
    "# YEAR_MONTH = datetime.today().strftime(\"%Y-%m\")\n",
    "YEAR_MONTH = \"2019-10\"\n",
    "# Assuming all data is inside us-visa-data folder\n",
    "DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR', '../us-visa-data')\n",
    "PARQUET_DIR = DOWNLOAD_DIR + \"/\" + FOLDER_NAME + \"/\" + YEAR_MONTH + \"/downloads/parquet\"\n",
    "PARQUET_MASTER_SCHEMA = PARQUET_DIR + \"/master_schema\"\n",
    "\n",
    "MASTER_SCHEMA = ['YEAR_CASE_BELONGS_TO','CASE_NUMBER','CASE_STATUS','CASE_SUBMITTED','DECISION_DATE','VISA_CLASS','EMPLOYMENT_START_DATE','EMPLOYMENT_END_DATE','EMPLOYER_NAME','EMPLOYER_ADDRESS','EMPLOYER_CITY','EMPLOYER_STATE','EMPLOYER_POSTAL_CODE','EMPLOYER_COUNTRY','EMPLOYER_PROVINCE','EMPLOYER_PHONE','EMPLOYER_PHONE_EXT','AGENT_REPRESENTING_EMPLOYER','AGENT_ATTORNEY_NAME','AGENT_ATTORNEY_CITY','AGENT_ATTORNEY_STATE','JOB_TITLE','SOC_CODE','SOC_NAME','NAICS_CODE','TOTAL_WORKERS','FULL_TIME_POSITION','PREVAILING_WAGE','PW_UNIT_OF_PAY','WAGE_RATE_OF_PAY_FROM','WAGE_RATE_OF_PAY_TO','WAGE_UNIT_OF_PAY_FROM','WAGE_UNIT_OF_PAY_TO','H1B_DEPENDENT','WILLFUL_VIOLATOR','WORKSITE_CITY','WORKSITE_COUNTY','WORKSITE_STATE','WORKSITE_POSTAL_CODE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning 2019 data with master schema\n",
    "- Add year that the dataset belongs to\n",
    "- Have wage_unit_of_pay split into from & to, because other years datasets have so\n",
    "- Drop wage_unit_of_pay after splitting into from & to columns\n",
    "- Extract only the MASTER_SCHEMA from the dataframe and save it back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2019.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2019.snappy.parquet into dataframe\n",
      "        YEAR_CASE_BELONGS_TO         CASE_NUMBER CASE_STATUS CASE_SUBMITTED  \\\n",
      "0                       2019  I-200-18323-885516      DENIED     2018-11-19   \n",
      "1                       2019  I-200-18323-882044      DENIED     2018-11-19   \n",
      "2                       2019  I-200-18356-858057      DENIED     2018-12-22   \n",
      "3                       2019  I-200-19081-827349   CERTIFIED     2019-03-22   \n",
      "4                       2019  I-203-19066-246476      DENIED     2019-03-07   \n",
      "...                      ...                 ...         ...            ...   \n",
      "552331                  2019  I-203-18272-864846   CERTIFIED     2018-09-29   \n",
      "552332                  2019  I-203-18273-010932   CERTIFIED     2018-09-30   \n",
      "552333                  2019  I-203-18273-978939   CERTIFIED     2018-09-30   \n",
      "552334                  2019  I-203-18323-409260   WITHDRAWN     2018-11-20   \n",
      "552335                  2019  I-200-18344-484841   WITHDRAWN     2018-12-10   \n",
      "\n",
      "       DECISION_DATE      VISA_CLASS EMPLOYMENT_START_DATE  \\\n",
      "0         2018-11-26            H-1B   2018-11-27 00:00:00   \n",
      "1         2018-11-26            H-1B   2018-11-28 00:00:00   \n",
      "2         2018-12-27            H-1B   2019-01-01 00:00:00   \n",
      "3         2019-03-28            H-1B   2019-04-01 00:00:00   \n",
      "4         2019-03-14  E-3 Australian   2019-04-08 00:00:00   \n",
      "...              ...             ...                   ...   \n",
      "552331    2018-10-04  E-3 Australian   2018-12-05 00:00:00   \n",
      "552332    2018-10-04  E-3 Australian   2018-11-23 00:00:00   \n",
      "552333    2018-10-04  E-3 Australian   2019-01-01 00:00:00   \n",
      "552334    2018-11-21  E-3 Australian   2018-11-26 00:00:00   \n",
      "552335    2018-12-10            H-1B   2018-12-10 00:00:00   \n",
      "\n",
      "        EMPLOYMENT_END_DATE                            EMPLOYER_NAME  \\\n",
      "0       2021-11-26 00:00:00            UNIVERSITY OF TEXAS AT DALLAS   \n",
      "1       2021-11-27 00:00:00            UNIVERSITY OF TEXAS AT DALLAS   \n",
      "2       2022-01-01 00:00:00                         BILLY R. WINGARD   \n",
      "3       2022-04-01 00:00:00                  COATES,BRIGHT & BAY LLC   \n",
      "4       2021-04-07 00:00:00          BOWLING GREEN CHRISTIAN ACADEMY   \n",
      "...                     ...                                      ...   \n",
      "552331  2020-11-05 00:00:00                           RESPONDENT INC   \n",
      "552332  2020-11-23 00:00:00  INTEGRATED CONTROLS AND INFORMATION LLC   \n",
      "552333  2019-12-31 00:00:00          TAYLOR VETERINARY HOSPITAL INC.   \n",
      "552334  2020-11-25 00:00:00            BLUEPRINT RESEARCH GROUP, LLC   \n",
      "552335  2020-11-12 00:00:00                         THE TORIT SCHOOL   \n",
      "\n",
      "               EMPLOYER_ADDRESS  ... WAGE_RATE_OF_PAY_FROM  \\\n",
      "0             800 W CAMPBELL RD  ...                 50000   \n",
      "1             800 W CAMPBELL RD  ...                207777   \n",
      "2           4705 EDGEWOOD DRIVE  ...                  7.95   \n",
      "3        2820 PAN AMERICAN BLVD  ...                  8.46   \n",
      "4             1730 DESTINY LANE  ...                   8.5   \n",
      "...                         ...  ...                   ...   \n",
      "552331          14 DUNHAM PLACE  ...                  None   \n",
      "552332  1092 JOHNNIE DODDS BLVD  ...                  None   \n",
      "552333    1231 WEST TAYLOR ROAD  ...                  None   \n",
      "552334       600 ALEXANDER ROAD  ...                  None   \n",
      "552335     300 CAMBRIDGE STREET  ...                  None   \n",
      "\n",
      "       WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY_FROM WAGE_UNIT_OF_PAY_TO  \\\n",
      "0                        0                  Year                Year   \n",
      "1                        0                  Year                Year   \n",
      "2                     7.95                  Hour                Hour   \n",
      "3                       10                  Hour                Hour   \n",
      "4                    10.25                  Hour                Hour   \n",
      "...                    ...                   ...                 ...   \n",
      "552331                None                  None                None   \n",
      "552332                None                  None                None   \n",
      "552333                None                  None                None   \n",
      "552334                None                  None                None   \n",
      "552335                None                  None                None   \n",
      "\n",
      "       H1B_DEPENDENT WILLFUL_VIOLATOR  WORKSITE_CITY WORKSITE_COUNTY  \\\n",
      "0                  N                N         Dallas          Dallas   \n",
      "1                  N                N     Richardson          Dallas   \n",
      "2                  N                N       New Bern          Craven   \n",
      "3                  Y                N     NORTH PORT         florida   \n",
      "4               None             None  BOWLING GREEN              19   \n",
      "...              ...              ...            ...             ...   \n",
      "552331          None             None           None            None   \n",
      "552332          None             None           None            None   \n",
      "552333          None             None           None            None   \n",
      "552334          None             None           None            None   \n",
      "552335             N                N           None            None   \n",
      "\n",
      "        WORKSITE_STATE WORKSITE_POSTAL_CODE  \n",
      "0                TEXAS                75235  \n",
      "1                TEXAS                75080  \n",
      "2       NORTH CAROLINA                28562  \n",
      "3              FLORIDA                34287  \n",
      "4             KENTUCKY                42104  \n",
      "...                ...                  ...  \n",
      "552331            None                 None  \n",
      "552332            None                 None  \n",
      "552333            None                 None  \n",
      "552334            None                 None  \n",
      "552335            None                 None  \n",
      "\n",
      "[552336 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2019\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Disclosure_Data_FY2019.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2019\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['WAGE_UNIT_OF_PAY']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['WAGE_UNIT_OF_PAY']\n",
    "df.drop(columns=['WAGE_UNIT_OF_PAY'],inplace=True)\n",
    "df_2019 = df[MASTER_SCHEMA]\n",
    "if (df_2019.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2019.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2019.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2019.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2019.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2019.snappy.parquet\"))\n",
    "print(df_new)\n",
    "del df, df_2019, df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning 2018 data with master schema\n",
    "- Add year that the dataset belongs to\n",
    "- Have wage_unit_of_pay split into from & to, because other years datasets have so\n",
    "- Drop wage_unit_of_pay after splitting into from & to columns\n",
    "- Extract only the MASTER_SCHEMA from the dataframe and save it back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2018.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2018.snappy.parquet into dataframe\n",
      "        YEAR_CASE_BELONGS_TO         CASE_NUMBER CASE_STATUS CASE_SUBMITTED  \\\n",
      "0                       2018  I-200-18026-338377   CERTIFIED     2018-01-29   \n",
      "1                       2018  I-200-17296-353451   CERTIFIED     2017-10-23   \n",
      "2                       2018  I-200-18242-524477   CERTIFIED     2018-08-30   \n",
      "3                       2018  I-200-18070-575236   CERTIFIED            NaT   \n",
      "4                       2018  I-200-18243-850522   CERTIFIED     2018-08-31   \n",
      "...                      ...                 ...         ...            ...   \n",
      "654355                  2018  I-200-18080-658767   CERTIFIED     2018-03-21   \n",
      "654356                  2018  I-200-18061-209836   CERTIFIED     2018-03-05   \n",
      "654357                  2018  I-201-17315-187517   CERTIFIED     2017-11-13   \n",
      "654358                  2018  I-200-18080-929219   CERTIFIED     2018-03-21   \n",
      "654359                  2018  I-200-18261-995519   CERTIFIED     2018-09-18   \n",
      "\n",
      "       DECISION_DATE   VISA_CLASS EMPLOYMENT_START_DATE EMPLOYMENT_END_DATE  \\\n",
      "0         2018-02-02         H-1B            2018-07-28          2021-07-27   \n",
      "1         2017-10-27         H-1B            2017-11-06          2020-11-06   \n",
      "2         2018-09-06         H-1B            2018-09-10          2021-09-09   \n",
      "3         2018-03-30         H-1B            2018-09-10          2021-09-09   \n",
      "4         2018-09-07         H-1B            2018-09-07          2021-09-06   \n",
      "...              ...          ...                   ...                 ...   \n",
      "654355    2018-03-27         H-1B            2018-09-09          2021-09-08   \n",
      "654356    2018-03-09         H-1B            2018-08-27          2021-08-27   \n",
      "654357    2017-11-17  H-1B1 Chile            2017-12-01          2018-12-01   \n",
      "654358    2018-03-27         H-1B            2018-04-01          2019-05-01   \n",
      "654359    2018-09-24         H-1B            2019-03-20          2022-03-19   \n",
      "\n",
      "                      EMPLOYER_NAME                EMPLOYER_ADDRESS  ...  \\\n",
      "0             MICROSOFT CORPORATION                 1 MICROSOFT WAY  ...   \n",
      "1            ERNST & YOUNG U.S. LLP                 200 PLAZA DRIVE  ...   \n",
      "2                      LOGIXHUB LLC                320 DECKER DRIVE  ...   \n",
      "3       HEXAWARE TECHNOLOGIES, INC.           101 WOOD AVENUE SOUTH  ...   \n",
      "4                  ECLOUD LABS,INC.               120 S WOOD AVENUE  ...   \n",
      "...                             ...                             ...  ...   \n",
      "654355       CROCKETT TECHNICAL LLC  2300 VALLEY VIEW LANE STE 1050  ...   \n",
      "654356        BRUNSWICK CORPORATION       26125 N. RIVERWOODS BLVD.  ...   \n",
      "654357             JOFEMAR USA INC.               2200 NW 102ND AVE  ...   \n",
      "654358    IBM INDIA PRIVATE LIMITED            3039 CORNWALLIS ROAD  ...   \n",
      "654359        MICROSOFT CORPORATION                 1 MICROSOFT WAY  ...   \n",
      "\n",
      "       WAGE_RATE_OF_PAY_FROM WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY_FROM  \\\n",
      "0                     143915                   0                  Year   \n",
      "1                     100000                   0                  Year   \n",
      "2                      78240                   0                  Year   \n",
      "3                      84406               85000                  Year   \n",
      "4                      95000                   0                  Year   \n",
      "...                      ...                 ...                   ...   \n",
      "654355                 91000                   0                  Year   \n",
      "654356                 74000                   0                  Year   \n",
      "654357                 65936                   0                  Year   \n",
      "654358                 81837                   0                  Year   \n",
      "654359                154381                   0                  Year   \n",
      "\n",
      "       WAGE_UNIT_OF_PAY_TO H1B_DEPENDENT WILLFUL_VIOLATOR WORKSITE_CITY  \\\n",
      "0                     Year             N                N       REDMOND   \n",
      "1                     Year             N                N   SANTA CLARA   \n",
      "2                     Year             N                N        IRVING   \n",
      "3                     Year             Y                N    NEW CASTLE   \n",
      "4                     Year             Y                N    BIRMINGHAM   \n",
      "...                    ...           ...              ...           ...   \n",
      "654355                Year             Y                N        IRVING   \n",
      "654356                Year             N                N   FOND DU LAC   \n",
      "654357                Year          None             None         DORAL   \n",
      "654358                Year             N                N          CARY   \n",
      "654359                Year             N                N       REDMOND   \n",
      "\n",
      "       WORKSITE_COUNTY WORKSITE_STATE WORKSITE_POSTAL_CODE  \n",
      "0                 KING             WA                98052  \n",
      "1             SAN JOSE             CA                95110  \n",
      "2               DALLAS             TX                75062  \n",
      "3           NEW CASTLE             DE                19720  \n",
      "4               SHELBY             AL                35244  \n",
      "...                ...            ...                  ...  \n",
      "654355          DALLAS             TX                75063  \n",
      "654356     FOND DU LAC             WI                54935  \n",
      "654357      MIAMI-DADE             FL                33172  \n",
      "654358            WAKE             NC                27513  \n",
      "654359            KING             WA                98052  \n",
      "\n",
      "[654360 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2018\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Disclosure_Data_FY2018_EOY.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2018\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['WAGE_UNIT_OF_PAY']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['WAGE_UNIT_OF_PAY']\n",
    "df.drop(columns=['WAGE_UNIT_OF_PAY'],inplace=True)\n",
    "df_2018 = df[MASTER_SCHEMA]\n",
    "if (df_2018.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2018.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2018.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2018.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2018.snappy.parquet\"))\n",
    "print(df_new)\n",
    "del df, df_2018, df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2017.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2017.snappy.parquet into dataframe\n",
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2017to2018.snappy.parquet\n",
      "Current dataset #:(624650, 39), cumulative dataset #: (1279010, 39)\n"
     ]
    }
   ],
   "source": [
    "# 2017\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Disclosure_Data_FY17.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2017\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['WAGE_UNIT_OF_PAY']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['WAGE_UNIT_OF_PAY']\n",
    "df.drop(columns=['WAGE_UNIT_OF_PAY'],inplace=True)\n",
    "df_2017 = df[MASTER_SCHEMA]\n",
    "if (df_2017.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2017.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2017.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2017.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2017.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2017.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "df_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2018.snappy.parquet\"))\n",
    "df_2017_to_2018 = pd.concat([df_2018,df_2017])\n",
    "df_2017_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2017to2018.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2017to2018.snappy.parquet\"))\n",
    "print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2017.shape,df_2017_to_2018.shape))\n",
    "\n",
    "del df, df_2018, df_2017, df_new, df_2017_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2016.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2016.snappy.parquet into dataframe\n",
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2016to2018.snappy.parquet\n",
      "Current dataset #:(647852, 39), cumulative dataset #: (1926862, 39)\n"
     ]
    }
   ],
   "source": [
    "# 2016\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Disclosure_Data_FY16.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2016\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['WAGE_UNIT_OF_PAY']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['WAGE_UNIT_OF_PAY']\n",
    "df.drop(columns=['WAGE_UNIT_OF_PAY'],inplace=True)\n",
    "\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2016 = df[MASTER_SCHEMA]\n",
    "if (df_2016.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2016.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2016.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2016.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "df_2017_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2017to2018.snappy.parquet\"))\n",
    "df_2016_to_2018 = pd.concat([df_2017_to_2018,df_2016])\n",
    "df_2016_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2016to2018.snappy.parquet\"))\n",
    "print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2016.shape,df_2016_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2016, df_2017_to_2018, df_2016_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2015.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2015.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2015\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Disclosure_Data_FY15_Q4.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2015\n",
    "\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "\n",
    "df['H1B_DEPENDENT'] = df['H-1B_DEPENDENT']\n",
    "df.drop(columns=['H-1B_DEPENDENT'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['NAIC_CODE']\n",
    "df.drop(columns=['NAIC_CODE'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_OF_PAY']\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WAGE_RATE_OF_PAY']\n",
    "df.drop(columns=['WAGE_RATE_OF_PAY'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['WAGE_UNIT_OF_PAY']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['WAGE_UNIT_OF_PAY']\n",
    "df.drop(columns=['WAGE_UNIT_OF_PAY'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['TOTAL WORKERS']\n",
    "df.drop(columns=['TOTAL WORKERS'],inplace=True)\n",
    "\n",
    "df['WILLFUL_VIOLATOR'] = df['WILLFUL VIOLATOR']\n",
    "df.drop(columns=['WILLFUL VIOLATOR'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EMPLOYER_ADDRESS1']\n",
    "df.drop(columns=['EMPLOYER_ADDRESS1'],inplace=True)\n",
    "\n",
    "\n",
    "df_2015 = df[MASTER_SCHEMA]\n",
    "if (df_2015.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2015.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2015.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "# del df, df_new , df_2015, df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2014.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2014.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2014\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_FY14_Q4.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2014\n",
    "\n",
    "df['CASE_NUMBER'] = df['LCA_CASE_NUMBER']\n",
    "df.drop(columns=['LCA_CASE_NUMBER'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['STATUS']\n",
    "df.drop(columns=['STATUS'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['LCA_CASE_SUBMIT']\n",
    "df.drop(columns=['LCA_CASE_SUBMIT'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['LCA_CASE_EMPLOYMENT_START_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_START_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['LCA_CASE_EMPLOYMENT_END_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_END_DATE'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['LCA_CASE_EMPLOYER_NAME']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['LCA_CASE_EMPLOYER_ADDRESS']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_ADDRESS'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['LCA_CASE_EMPLOYER_CITY']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_CITY'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['LCA_CASE_EMPLOYER_STATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['LCA_CASE_EMPLOYER_POSTAL_CODE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "\n",
    "\n",
    "df['JOB_TITLE'] = df['LCA_CASE_JOB_TITLE']\n",
    "df.drop(columns=['LCA_CASE_JOB_TITLE'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['LCA_CASE_SOC_CODE']\n",
    "df.drop(columns=['LCA_CASE_SOC_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['LCA_CASE_SOC_NAME']\n",
    "df.drop(columns=['LCA_CASE_SOC_NAME'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['LCA_CASE_NAICS_CODE']\n",
    "df.drop(columns=['LCA_CASE_NAICS_CODE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['FULL_TIME_POS']\n",
    "df.drop(columns=['FULL_TIME_POS'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PW_1']\n",
    "df.drop(columns=['PW_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_1']\n",
    "df.drop(columns=['PW_UNIT_1'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_FROM']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_FROM'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_TO']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_TO'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_UNIT'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['LCA_CASE_WORKLOC1_CITY']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_CITY'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['LCA_CASE_WORKLOC1_STATE']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_STATE'],inplace=True)\n",
    "\n",
    "\n",
    "df_2014 = df[MASTER_SCHEMA]\n",
    "if (df_2014.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2014.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2014.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2014.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2014.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2014.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2014 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2013.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2013.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2013\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'LCA_FY2013.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2013\n",
    "\n",
    "df['DECISION_DATE'] = df['Decision_Date']\n",
    "df.drop(columns=['Decision_Date'],inplace=True)\n",
    "\n",
    "df['CASE_NUMBER'] = df['LCA_CASE_NUMBER']\n",
    "df.drop(columns=['LCA_CASE_NUMBER'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['STATUS']\n",
    "df.drop(columns=['STATUS'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['LCA_CASE_SUBMIT']\n",
    "df.drop(columns=['LCA_CASE_SUBMIT'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['LCA_CASE_EMPLOYMENT_START_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_START_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['LCA_CASE_EMPLOYMENT_END_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_END_DATE'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['LCA_CASE_EMPLOYER_NAME']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['LCA_CASE_EMPLOYER_ADDRESS']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_ADDRESS'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['LCA_CASE_EMPLOYER_CITY']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_CITY'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['LCA_CASE_EMPLOYER_STATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['LCA_CASE_EMPLOYER_POSTAL_CODE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "\n",
    "\n",
    "df['JOB_TITLE'] = df['LCA_CASE_JOB_TITLE']\n",
    "df.drop(columns=['LCA_CASE_JOB_TITLE'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['LCA_CASE_SOC_CODE']\n",
    "df.drop(columns=['LCA_CASE_SOC_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['LCA_CASE_SOC_NAME']\n",
    "df.drop(columns=['LCA_CASE_SOC_NAME'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['LCA_CASE_NAICS_CODE']\n",
    "df.drop(columns=['LCA_CASE_NAICS_CODE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['FULL_TIME_POS']\n",
    "df.drop(columns=['FULL_TIME_POS'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PW_1']\n",
    "df.drop(columns=['PW_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_1']\n",
    "df.drop(columns=['PW_UNIT_1'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_FROM']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_FROM'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_TO']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_TO'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_UNIT'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['LCA_CASE_WORKLOC1_CITY']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_CITY'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['LCA_CASE_WORKLOC1_STATE']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_STATE'],inplace=True)\n",
    "\n",
    "\n",
    "df_2013 = df[MASTER_SCHEMA]\n",
    "if (df_2013.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2013.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2013.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2013.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2013.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2013.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2013 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2012.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2012.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2012\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'LCA_FY2012_Q4.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2012\n",
    "\n",
    "df['CASE_NUMBER'] = df['LCA_CASE_NUMBER']\n",
    "df.drop(columns=['LCA_CASE_NUMBER'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['STATUS']\n",
    "df.drop(columns=['STATUS'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['LCA_CASE_SUBMIT']\n",
    "df.drop(columns=['LCA_CASE_SUBMIT'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['LCA_CASE_EMPLOYMENT_START_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_START_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['LCA_CASE_EMPLOYMENT_END_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_END_DATE'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['LCA_CASE_EMPLOYER_NAME']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['LCA_CASE_EMPLOYER_ADDRESS']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_ADDRESS'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['LCA_CASE_EMPLOYER_CITY']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_CITY'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['LCA_CASE_EMPLOYER_STATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['LCA_CASE_EMPLOYER_POSTAL_CODE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "\n",
    "\n",
    "df['JOB_TITLE'] = df['LCA_CASE_JOB_TITLE']\n",
    "df.drop(columns=['LCA_CASE_JOB_TITLE'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['LCA_CASE_SOC_CODE']\n",
    "df.drop(columns=['LCA_CASE_SOC_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['LCA_CASE_SOC_NAME']\n",
    "df.drop(columns=['LCA_CASE_SOC_NAME'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['LCA_CASE_NAICS_CODE']\n",
    "df.drop(columns=['LCA_CASE_NAICS_CODE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['FULL_TIME_POS']\n",
    "df.drop(columns=['FULL_TIME_POS'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PW_1']\n",
    "df.drop(columns=['PW_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_1']\n",
    "df.drop(columns=['PW_UNIT_1'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_FROM']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_FROM'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_TO']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_TO'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_UNIT'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['LCA_CASE_WORKLOC1_CITY']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_CITY'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['LCA_CASE_WORKLOC1_STATE']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_STATE'],inplace=True)\n",
    "\n",
    "\n",
    "df_2012 = df[MASTER_SCHEMA]\n",
    "if (df_2012.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2012.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2012.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2012.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2012.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2012.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2012 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2011.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2011.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2011\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_iCert_LCA_FY2011_Q4.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2011\n",
    "\n",
    "df['CASE_NUMBER'] = df['LCA_CASE_NUMBER']\n",
    "df.drop(columns=['LCA_CASE_NUMBER'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['STATUS']\n",
    "df.drop(columns=['STATUS'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['LCA_CASE_SUBMIT']\n",
    "df.drop(columns=['LCA_CASE_SUBMIT'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['LCA_CASE_EMPLOYMENT_START_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_START_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['LCA_CASE_EMPLOYMENT_END_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_END_DATE'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['LCA_CASE_EMPLOYER_NAME']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['LCA_CASE_EMPLOYER_ADDRESS']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_ADDRESS'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['LCA_CASE_EMPLOYER_CITY']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_CITY'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['LCA_CASE_EMPLOYER_STATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['LCA_CASE_EMPLOYER_POSTAL_CODE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "\n",
    "\n",
    "df['JOB_TITLE'] = df['LCA_CASE_JOB_TITLE']\n",
    "df.drop(columns=['LCA_CASE_JOB_TITLE'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['LCA_CASE_SOC_CODE']\n",
    "df.drop(columns=['LCA_CASE_SOC_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['LCA_CASE_SOC_NAME']\n",
    "df.drop(columns=['LCA_CASE_SOC_NAME'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['LCA_CASE_NAICS_CODE']\n",
    "df.drop(columns=['LCA_CASE_NAICS_CODE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['FULL_TIME_POS']\n",
    "df.drop(columns=['FULL_TIME_POS'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PW_1']\n",
    "df.drop(columns=['PW_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_1']\n",
    "df.drop(columns=['PW_UNIT_1'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_FROM']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_FROM'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_TO']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_TO'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_UNIT'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['LCA_CASE_WORKLOC1_CITY']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_CITY'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['LCA_CASE_WORKLOC1_STATE']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_STATE'],inplace=True)\n",
    "\n",
    "\n",
    "df_2011 = df[MASTER_SCHEMA]\n",
    "if (df_2011.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2011.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2011.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2011.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2011.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2011.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2011 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2010.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2010.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2010\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_FY2010.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2010\n",
    "\n",
    "df['CASE_NUMBER'] = df['LCA_CASE_NUMBER']\n",
    "df.drop(columns=['LCA_CASE_NUMBER'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['STATUS']\n",
    "df.drop(columns=['STATUS'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['LCA_CASE_SUBMIT']\n",
    "df.drop(columns=['LCA_CASE_SUBMIT'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['LCA_CASE_EMPLOYMENT_START_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_START_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['LCA_CASE_EMPLOYMENT_END_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_END_DATE'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['LCA_CASE_EMPLOYER_NAME']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['LCA_CASE_EMPLOYER_ADDRESS1']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_ADDRESS1'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['LCA_CASE_EMPLOYER_CITY']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_CITY'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['LCA_CASE_EMPLOYER_STATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['LCA_CASE_EMPLOYER_POSTAL_CODE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['FULL_TIME_POSITION'] = np.nan\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = np.nan\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = np.nan\n",
    "\n",
    "\n",
    "df['JOB_TITLE'] = df['LCA_CASE_JOB_TITLE']\n",
    "df.drop(columns=['LCA_CASE_JOB_TITLE'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['LCA_CASE_SOC_CODE']\n",
    "df.drop(columns=['LCA_CASE_SOC_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['LCA_CASE_SOC_NAME']\n",
    "df.drop(columns=['LCA_CASE_SOC_NAME'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['LCA_CASE_NAICS_CODE']\n",
    "df.drop(columns=['LCA_CASE_NAICS_CODE'],inplace=True)\n",
    "\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PW_1']\n",
    "df.drop(columns=['PW_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_1']\n",
    "df.drop(columns=['PW_UNIT_1'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_FROM']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_FROM'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_TO']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_TO'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WORK_LOCATION_CITY1']\n",
    "df.drop(columns=['WORK_LOCATION_CITY1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WORK_LOCATION_STATE1']\n",
    "df.drop(columns=['WORK_LOCATION_STATE1'],inplace=True)\n",
    "\n",
    "\n",
    "df_2010 = df[MASTER_SCHEMA]\n",
    "if (df_2010.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2010.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2010.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2010.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2010.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2010.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2010 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2009_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2009_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2009-1\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'Icert_ LCA_ FY2009.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2009\n",
    "\n",
    "df['DECISION_DATE'] = df['Decision_Date']\n",
    "df.drop(columns=['Decision_Date'],inplace=True)\n",
    "\n",
    "df['CASE_NUMBER'] = df['LCA_CASE_NUMBER']\n",
    "df.drop(columns=['LCA_CASE_NUMBER'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['STATUS']\n",
    "df.drop(columns=['STATUS'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['LCA_CASE_SUBMIT']\n",
    "df.drop(columns=['LCA_CASE_SUBMIT'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['LCA_CASE_EMPLOYMENT_START_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_START_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['LCA_CASE_EMPLOYMENT_END_DATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYMENT_END_DATE'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['LCA_CASE_EMPLOYER_NAME']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['LCA_CASE_EMPLOYER_ADDRESS']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_ADDRESS'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['LCA_CASE_EMPLOYER_CITY']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_CITY'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['LCA_CASE_EMPLOYER_STATE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['LCA_CASE_EMPLOYER_POSTAL_CODE']\n",
    "df.drop(columns=['LCA_CASE_EMPLOYER_POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['FULL_TIME_POSITION'] = np.nan\n",
    "\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_UNIT']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_UNIT'],inplace=True)\n",
    "\n",
    "\n",
    "df['JOB_TITLE'] = df['LCA_CASE_JOB_TITLE']\n",
    "df.drop(columns=['LCA_CASE_JOB_TITLE'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['LCA_CASE_SOC_CODE']\n",
    "df.drop(columns=['LCA_CASE_SOC_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['LCA_CASE_SOC_NAME']\n",
    "df.drop(columns=['LCA_CASE_SOC_NAME'],inplace=True)\n",
    "\n",
    "df['NAICS_CODE'] = df['LCA_CASE_NAICS_CODE']\n",
    "df.drop(columns=['LCA_CASE_NAICS_CODE'],inplace=True)\n",
    "\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PW_1']\n",
    "df.drop(columns=['PW_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_1']\n",
    "df.drop(columns=['PW_UNIT_1'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['LCA_CASE_WAGE_RATE_FROM']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_FROM'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['LCA_CASE_WAGE_RATE_TO']\n",
    "df.drop(columns=['LCA_CASE_WAGE_RATE_TO'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['LCA_CASE_WORKLOC1_CITY']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_CITY'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['LCA_CASE_WORKLOC1_STATE']\n",
    "df.drop(columns=['LCA_CASE_WORKLOC1_STATE'],inplace=True)\n",
    "\n",
    "\n",
    "df_2009_1 = df[MASTER_SCHEMA]\n",
    "if (df_2009_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2009_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2009_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2009_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2009_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2009_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2009_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2009_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2009_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2009-2\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Case_Data_FY2009.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2009\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = df['PROGRAM_DESIGNATION']\n",
    "df.drop(columns=['PROGRAM_DESIGNATION'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EMPLOYER_ADDRESS1'] + df['EMPLOYER_ADDRESS2']\n",
    "df.drop(columns=['EMPLOYER_ADDRESS1','EMPLOYER_ADDRESS2'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WAGE_RATE__2']\n",
    "df.drop(columns=['WAGE_RATE__2'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_2']\n",
    "df.drop(columns=['RATE_PER_2'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['OCCUPATIONAL_CODE']\n",
    "df.drop(columns=['OCCUPATIONAL_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['OCCUPATIONAL_TITLE']\n",
    "df.drop(columns=['OCCUPATIONAL_TITLE'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_2']\n",
    "df.drop(columns=['CITY_2'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "\n",
    "df_2009_2 = df[MASTER_SCHEMA]\n",
    "if (df_2009_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2009_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2009_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2009_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2009_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2009_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2009_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2008.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2008.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2008\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H-1B_Case_Data_FY2008.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2008\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = df['PROGRAM']\n",
    "df.drop(columns=['PROGRAM'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['NAME']\n",
    "df.drop(columns=['NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['ADDRESS1']\n",
    "df.drop(columns=['ADDRESS1'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['CITY']\n",
    "df.drop(columns=['CITY'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['STATE']\n",
    "df.drop(columns=['STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['POSTAL_CODE']\n",
    "df.drop(columns=['POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WAGE_RATE_2']\n",
    "df.drop(columns=['WAGE_RATE_2'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_2']\n",
    "df.drop(columns=['RATE_PER_2'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JOB_CODE']\n",
    "df.drop(columns=['JOB_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = df['OCCUPATIONAL_TITLE']\n",
    "df.drop(columns=['OCCUPATIONAL_TITLE'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_2']\n",
    "df.drop(columns=['CITY_2'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "\n",
    "df_2008 = df[MASTER_SCHEMA]\n",
    "if (df_2008.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2008.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2008.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2008.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2008.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2008.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2008 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2007_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2007_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2007 efile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'EFILE_FY2007.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2007\n",
    "\n",
    "df['JOB_TITLE'] = df['Job_Title']\n",
    "df.drop(columns=['Job_Title'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_Decision_Date']\n",
    "df.drop(columns=['DOL_Decision_Date'],inplace=True)\n",
    "\n",
    "df['CASE_NUMBER'] = df['Case Number']\n",
    "df.drop(columns=['Case Number'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['Case_Status']\n",
    "df.drop(columns=['Case_Status'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = df['Program Designation']\n",
    "df.drop(columns=['Program Designation'],inplace=True)\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['Submitted_Date']\n",
    "df.drop(columns=['Submitted_Date'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['Employer_Name']\n",
    "df.drop(columns=['Employer_Name'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['Address_1'] + df['Address_2']\n",
    "df.drop(columns=['Address_1','Address_2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['City']\n",
    "df.drop(columns=['City'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['State']\n",
    "df.drop(columns=['State'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['Zip_Code']\n",
    "df.drop(columns=['Zip_Code'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['Nbr_Immigrants']\n",
    "df.drop(columns=['Nbr_Immigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['Begin_Date']\n",
    "df.drop(columns=['Begin_Date'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['End_Date']\n",
    "df.drop(columns=['End_Date'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['Part_Time_1']\n",
    "df.drop(columns=['Part_Time_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['Prevailing_Wage_1']\n",
    "df.drop(columns=['Prevailing_Wage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['Wage_Rate_From_1']\n",
    "df.drop(columns=['Wage_Rate_From_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['Wage_Rate_To_1']\n",
    "df.drop(columns=['Wage_Rate_To_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['Wage_Rate_Per_1']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['Wage_Rate_Per_1']\n",
    "df.drop(columns=['Wage_Rate_Per_1'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['Occupation_Code']\n",
    "df.drop(columns=['Occupation_Code'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['WORKSITE_CITY'] = df['Work_City_1']\n",
    "df.drop(columns=['Work_City_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['Work_State_1']\n",
    "df.drop(columns=['Work_State_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2007_1 = df[MASTER_SCHEMA]\n",
    "if (df_2007_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2007_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2007_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2007_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2007_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2007_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2007_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2006_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2006_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2006 efile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_efile_FY06.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2006\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['NAME']\n",
    "df.drop(columns=['NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['ADDRESS'] + df['ADDRESS2']\n",
    "df.drop(columns=['ADDRESS','ADDRESS2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['CITY']\n",
    "df.drop(columns=['CITY'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['STATE']\n",
    "df.drop(columns=['STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['POSTAL_CODE']\n",
    "df.drop(columns=['POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['MAX_RATE_1']\n",
    "df.drop(columns=['MAX_RATE_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JOB_CODE']\n",
    "df.drop(columns=['JOB_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_1']\n",
    "df.drop(columns=['CITY_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2006_1 = df[MASTER_SCHEMA]\n",
    "if (df_2006_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2006_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2006_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2006_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2006_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2006_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2006_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2005_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2005_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2005 efile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_efile_FY05.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2005\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['NAME']\n",
    "df.drop(columns=['NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['ADDRESS'] + df['ADDRESS2']\n",
    "df.drop(columns=['ADDRESS','ADDRESS2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['CITY']\n",
    "df.drop(columns=['CITY'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['STATE']\n",
    "df.drop(columns=['STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['POSTAL_CODE']\n",
    "df.drop(columns=['POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['MAX_RATE_1']\n",
    "df.drop(columns=['MAX_RATE_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JOB_CODE']\n",
    "df.drop(columns=['JOB_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_1']\n",
    "df.drop(columns=['CITY_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2005_1 = df[MASTER_SCHEMA]\n",
    "if (df_2005_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2005_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2005_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2005_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2005_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2005_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2005_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2004_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2004_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2004 efile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_efile_FY04.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2004\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['NAME']\n",
    "df.drop(columns=['NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['ADDRESS'] + df['ADDRESS2']\n",
    "df.drop(columns=['ADDRESS','ADDRESS2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['CITY']\n",
    "df.drop(columns=['CITY'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['STATE']\n",
    "df.drop(columns=['STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['POSTAL_CODE']\n",
    "df.drop(columns=['POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['MAX_RATE_1']\n",
    "df.drop(columns=['MAX_RATE_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JOB_CODE']\n",
    "df.drop(columns=['JOB_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_1']\n",
    "df.drop(columns=['CITY_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2004_1 = df[MASTER_SCHEMA]\n",
    "if (df_2004_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2004_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2004_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2004_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2004_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2004_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2004_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2003_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2003_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2003 efile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_efile_FY03.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2003\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['NAME']\n",
    "df.drop(columns=['NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['ADDRESS'] + df['ADDRESS2']\n",
    "df.drop(columns=['ADDRESS','ADDRESS2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['CITY']\n",
    "df.drop(columns=['CITY'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['STATE']\n",
    "df.drop(columns=['STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['POSTAL_CODE']\n",
    "df.drop(columns=['POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['MAX_RATE_1']\n",
    "df.drop(columns=['MAX_RATE_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JOB_CODE']\n",
    "df.drop(columns=['JOB_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_1']\n",
    "df.drop(columns=['CITY_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2003_1 = df[MASTER_SCHEMA]\n",
    "if (df_2003_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2003_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2003_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2003_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2003_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2003_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2003_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2002_1.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2002_1.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2002 efile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_efile_FY02.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2002\n",
    "\n",
    "df['CASE_NUMBER'] = df['CASE_NO']\n",
    "df.drop(columns=['CASE_NO'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['APPROVAL_STATUS']\n",
    "df.drop(columns=['APPROVAL_STATUS'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['DOL_DECISION_DATE']\n",
    "df.drop(columns=['DOL_DECISION_DATE'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['SUBMITTED_DATE']\n",
    "df.drop(columns=['SUBMITTED_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['NAME']\n",
    "df.drop(columns=['NAME'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['ADDRESS'] + df['ADDRESS2']\n",
    "df.drop(columns=['ADDRESS','ADDRESS2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['CITY']\n",
    "df.drop(columns=['CITY'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['STATE']\n",
    "df.drop(columns=['STATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['POSTAL_CODE']\n",
    "df.drop(columns=['POSTAL_CODE'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NBR_IMMIGRANTS']\n",
    "df.drop(columns=['NBR_IMMIGRANTS'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BEGIN_DATE']\n",
    "df.drop(columns=['BEGIN_DATE'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['END_DATE']\n",
    "df.drop(columns=['END_DATE'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PART_TIME_1']\n",
    "df.drop(columns=['PART_TIME_1'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE_1']\n",
    "df.drop(columns=['PREVAILING_WAGE_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WAGE_RATE_1']\n",
    "df.drop(columns=['WAGE_RATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['MAX_RATE_1']\n",
    "df.drop(columns=['MAX_RATE_1'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RATE_PER_1']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RATE_PER_1']\n",
    "df.drop(columns=['RATE_PER_1'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JOB_CODE']\n",
    "df.drop(columns=['JOB_CODE'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['WORKSITE_CITY'] = df['CITY_1']\n",
    "df.drop(columns=['CITY_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['STATE_1']\n",
    "df.drop(columns=['STATE_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2002_1 = df[MASTER_SCHEMA]\n",
    "if (df_2002_1.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2002_1.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2002_1.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2002_1.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2002_1.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2002_1.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2002_1 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2006_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2006_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2006 fax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_Fax_FY2006_External_Web.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2006\n",
    "\n",
    "df['CASE_NUMBER'] = df['C_num']\n",
    "df.drop(columns=['C_num'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['CertCode']\n",
    "df.drop(columns=['CertCode'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['Det_Date']\n",
    "df.drop(columns=['Det_Date'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['DateSigned']\n",
    "df.drop(columns=['DateSigned'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['EmpName']\n",
    "df.drop(columns=['EmpName'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EmpAddy1'] + df['EmpAddy2']\n",
    "df.drop(columns=['EmpAddy1','EmpAddy2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['EmpCity']\n",
    "df.drop(columns=['EmpCity'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['EmpState']\n",
    "df.drop(columns=['EmpState'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['EmpZip']\n",
    "df.drop(columns=['EmpZip'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NumImmigrants']\n",
    "df.drop(columns=['NumImmigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BeginDate']\n",
    "df.drop(columns=['BeginDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['EndDate']\n",
    "df.drop(columns=['EndDate'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PartTime']\n",
    "df.drop(columns=['PartTime'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PrevWage_1']\n",
    "df.drop(columns=['PrevWage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WageRateFrom']\n",
    "df.drop(columns=['WageRateFrom'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WageRateTo']\n",
    "df.drop(columns=['WageRateTo'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RatePer']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RatePer']\n",
    "df.drop(columns=['RatePer'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JobCode']\n",
    "df.drop(columns=['JobCode'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['JOB_TITLE'] = df['JobTitle']\n",
    "df.drop(columns=['JobTitle'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WorkCity_1']\n",
    "df.drop(columns=['WorkCity_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WorkState_1']\n",
    "df.drop(columns=['WorkState_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2006_2 = df[MASTER_SCHEMA]\n",
    "if (df_2006_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2006_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2006_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2006_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2006_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2006_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2006_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2005_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2005_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2005 fax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_Fax_FY2005_Download.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2005\n",
    "\n",
    "df['CASE_NUMBER'] = df['C_num']\n",
    "df.drop(columns=['C_num'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['CertCode']\n",
    "df.drop(columns=['CertCode'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['Det_Date']\n",
    "df.drop(columns=['Det_Date'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['CertStart']\n",
    "df.drop(columns=['CertStart'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['EmpName']\n",
    "df.drop(columns=['EmpName'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EmpAddy1'] + df['EmpAddy2']\n",
    "df.drop(columns=['EmpAddy1','EmpAddy2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['EmpCity']\n",
    "df.drop(columns=['EmpCity'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['EmpState']\n",
    "df.drop(columns=['EmpState'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['EmpZip']\n",
    "df.drop(columns=['EmpZip'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NumImmigrants']\n",
    "df.drop(columns=['NumImmigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BeginDate']\n",
    "df.drop(columns=['BeginDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['EndDate']\n",
    "df.drop(columns=['EndDate'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PartTime']\n",
    "df.drop(columns=['PartTime'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PrevWage_1']\n",
    "df.drop(columns=['PrevWage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WageRateFrom']\n",
    "df.drop(columns=['WageRateFrom'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WageRateTo']\n",
    "df.drop(columns=['WageRateTo'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RatePer']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RatePer']\n",
    "df.drop(columns=['RatePer'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JobCode']\n",
    "df.drop(columns=['JobCode'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['JOB_TITLE'] = df['JobTitle']\n",
    "df.drop(columns=['JobTitle'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WorkCity_1']\n",
    "df.drop(columns=['WorkCity_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WorkState_1']\n",
    "df.drop(columns=['WorkState_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2005_2 = df[MASTER_SCHEMA]\n",
    "if (df_2005_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2005_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2005_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2005_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2005_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2005_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2005_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2004_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2004_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2004 fax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_fax_FY04.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2004\n",
    "\n",
    "df['CASE_NUMBER'] = df['Case Number']\n",
    "df.drop(columns=['Case Number'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['CertCode']\n",
    "df.drop(columns=['CertCode'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['Det_Date']\n",
    "df.drop(columns=['Det_Date'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['ProcessDate']\n",
    "df.drop(columns=['ProcessDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['EmpName']\n",
    "df.drop(columns=['EmpName'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EmpAddy1'] + df['EmpAddy2']\n",
    "df.drop(columns=['EmpAddy1','EmpAddy2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['EmpCity']\n",
    "df.drop(columns=['EmpCity'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['EmpState']\n",
    "df.drop(columns=['EmpState'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['EmpZip']\n",
    "df.drop(columns=['EmpZip'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NumImmigrants']\n",
    "df.drop(columns=['NumImmigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BeginDate']\n",
    "df.drop(columns=['BeginDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['EndDate']\n",
    "df.drop(columns=['EndDate'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PartTime?']\n",
    "df.drop(columns=['PartTime?'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PrevWage_1']\n",
    "df.drop(columns=['PrevWage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WageRateFrom']\n",
    "df.drop(columns=['WageRateFrom'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WageRateTo']\n",
    "df.drop(columns=['WageRateTo'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RatePer']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RatePer']\n",
    "df.drop(columns=['RatePer'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JobCode']\n",
    "df.drop(columns=['JobCode'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['JOB_TITLE'] = df['JobTitle']\n",
    "df.drop(columns=['JobTitle'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WorkCity_1']\n",
    "df.drop(columns=['WorkCity_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WorkState_1']\n",
    "df.drop(columns=['WorkState_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2004_2 = df[MASTER_SCHEMA]\n",
    "if (df_2004_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2004_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2004_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2004_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2004_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2004_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2004_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2003_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2003_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2003 fax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_Fax_FY2003_Download.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2003\n",
    "\n",
    "df['CASE_NUMBER'] = df['C_num']\n",
    "df.drop(columns=['C_num'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['CertCode']\n",
    "df.drop(columns=['CertCode'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['Det_Date']\n",
    "df.drop(columns=['Det_Date'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['ProcessDate']\n",
    "df.drop(columns=['ProcessDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['EmpName']\n",
    "df.drop(columns=['EmpName'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EmpAddy1'] + df['EmpAddy2']\n",
    "df.drop(columns=['EmpAddy1','EmpAddy2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['EmpCity']\n",
    "df.drop(columns=['EmpCity'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['EmpState']\n",
    "df.drop(columns=['EmpState'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['EmpZip']\n",
    "df.drop(columns=['EmpZip'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NumImmigrants']\n",
    "df.drop(columns=['NumImmigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BeginDate']\n",
    "df.drop(columns=['BeginDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['EndDate']\n",
    "df.drop(columns=['EndDate'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PartTime']\n",
    "df.drop(columns=['PartTime'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PrevWage_1']\n",
    "df.drop(columns=['PrevWage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WageRateFrom']\n",
    "df.drop(columns=['WageRateFrom'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WageRateTo']\n",
    "df.drop(columns=['WageRateTo'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RatePer']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RatePer']\n",
    "df.drop(columns=['RatePer'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JobCode']\n",
    "df.drop(columns=['JobCode'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['JOB_TITLE'] = df['JobTitle']\n",
    "df.drop(columns=['JobTitle'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WorkCity_1']\n",
    "df.drop(columns=['WorkCity_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WorkState_1']\n",
    "df.drop(columns=['WorkState_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2003_2 = df[MASTER_SCHEMA]\n",
    "if (df_2003_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2003_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2003_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2003_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2003_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2003_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2003_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2002_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2002_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2002 fax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_FAX_FY2002_Download.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2002\n",
    "\n",
    "df['CASE_NUMBER'] = df['C_num']\n",
    "df.drop(columns=['C_num'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['CertCode']\n",
    "df.drop(columns=['CertCode'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['Det_Date']\n",
    "df.drop(columns=['Det_Date'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['ProcessDate']\n",
    "df.drop(columns=['ProcessDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['EmpName']\n",
    "df.drop(columns=['EmpName'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EmpAddy1'] + df['EmpAddy2']\n",
    "df.drop(columns=['EmpAddy1','EmpAddy2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['EmpCity']\n",
    "df.drop(columns=['EmpCity'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['EmpState']\n",
    "df.drop(columns=['EmpState'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['EmpZip']\n",
    "df.drop(columns=['EmpZip'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NumImmigrants']\n",
    "df.drop(columns=['NumImmigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BeginDate']\n",
    "df.drop(columns=['BeginDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['EndDate']\n",
    "df.drop(columns=['EndDate'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PartTime']\n",
    "df.drop(columns=['PartTime'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PrevWage_1']\n",
    "df.drop(columns=['PrevWage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WageRateFrom']\n",
    "df.drop(columns=['WageRateFrom'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WageRateTo']\n",
    "df.drop(columns=['WageRateTo'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RatePer']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RatePer']\n",
    "df.drop(columns=['RatePer'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JobCode']\n",
    "df.drop(columns=['JobCode'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['JOB_TITLE'] = df['JobTitle']\n",
    "df.drop(columns=['JobTitle'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WorkCity_1']\n",
    "df.drop(columns=['WorkCity_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WorkState_1']\n",
    "df.drop(columns=['WorkState_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2002_2 = df[MASTER_SCHEMA]\n",
    "if (df_2002_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2002_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2002_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2002_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2002_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2002_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2002_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['C_num', 'CertCode', 'ReturnFax', 'EmpName', 'EmpCity', 'EmpAddy1',\n",
       "       'EmpAddy2', 'EmpState', 'EmpZip', 'WageRateFrom', 'WageRateTo',\n",
       "       'RatePer', 'PartTime', 'BeginDate', 'EndDate', 'JobCode',\n",
       "       'NumImmigrants', 'JobTitle', 'WorkCity_1', 'WorkState_1', 'PrevWage_1',\n",
       "       'PrevWagePer_1', 'WageSource_1', 'WorkYear1', 'OtherWageSource1',\n",
       "       'WorkCity2', 'WorkState2', 'PrevWage2', 'PrevWagePer_2', 'WageSource_2',\n",
       "       'WorkYear_2', 'OtherWageSource2', 'CertStart', 'CertEnd', 'Det_Date',\n",
       "       'ProcessDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_Fax_FY2001_Download.snappy.parquet'))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created: ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2001_2.snappy.parquet\n",
      "Reading ../us-visa-data/LCAProgramsH1BH1B1E3/2019-10/downloads/parquet/master_schema/2001_2.snappy.parquet into dataframe\n"
     ]
    }
   ],
   "source": [
    "# 2001 fax\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(PARQUET_DIR,'H1B_Fax_FY2001_Download.snappy.parquet'))\n",
    "df['YEAR_CASE_BELONGS_TO'] = 2001\n",
    "\n",
    "df['CASE_NUMBER'] = df['C_num']\n",
    "df.drop(columns=['C_num'],inplace=True)\n",
    "\n",
    "df['CASE_STATUS'] = df['CertCode']\n",
    "df.drop(columns=['CertCode'],inplace=True)\n",
    "\n",
    "df['DECISION_DATE'] = df['Det_Date']\n",
    "df.drop(columns=['Det_Date'],inplace=True)\n",
    "\n",
    "df['VISA_CLASS'] = np.nan\n",
    "\n",
    "df['CASE_SUBMITTED'] = df['ProcessDate']\n",
    "df.drop(columns=['ProcessDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_NAME'] = df['EmpName']\n",
    "df.drop(columns=['EmpName'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_ADDRESS'] = df['EmpAddy1'] + df['EmpAddy2']\n",
    "df.drop(columns=['EmpAddy1','EmpAddy2'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_CITY'] = df['EmpCity']\n",
    "df.drop(columns=['EmpCity'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_STATE'] = df['EmpState']\n",
    "df.drop(columns=['EmpState'],inplace=True)\n",
    "\n",
    "df['EMPLOYER_POSTAL_CODE'] = df['EmpZip']\n",
    "df.drop(columns=['EmpZip'],inplace=True)\n",
    "\n",
    "df['TOTAL_WORKERS'] = df['NumImmigrants']\n",
    "df.drop(columns=['NumImmigrants'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['BeginDate']\n",
    "df.drop(columns=['BeginDate'],inplace=True)\n",
    "\n",
    "df['EMPLOYMENT_END_DATE'] = df['EndDate']\n",
    "df.drop(columns=['EndDate'],inplace=True)\n",
    "\n",
    "df['FULL_TIME_POSITION'] = df['PartTime']\n",
    "df.drop(columns=['PartTime'],inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PrevWage_1']\n",
    "df.drop(columns=['PrevWage_1'],inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'] = np.nan\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = df['WageRateFrom']\n",
    "df.drop(columns=['WageRateFrom'],inplace=True)\n",
    "\n",
    "df['WAGE_RATE_OF_PAY_TO'] = df['WageRateTo']\n",
    "df.drop(columns=['WageRateTo'],inplace=True)\n",
    "\n",
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['RatePer']\n",
    "df['WAGE_UNIT_OF_PAY_TO'] = df['RatePer']\n",
    "df.drop(columns=['RatePer'],inplace=True)\n",
    "\n",
    "df['SOC_CODE'] = df['JobCode']\n",
    "df.drop(columns=['JobCode'],inplace=True)\n",
    "\n",
    "df['SOC_NAME'] = np.nan\n",
    "\n",
    "df['JOB_TITLE'] = df['JobTitle']\n",
    "df.drop(columns=['JobTitle'],inplace=True)\n",
    "\n",
    "df['WORKSITE_CITY'] = df['WorkCity_1']\n",
    "df.drop(columns=['WorkCity_1'],inplace=True)\n",
    "\n",
    "df['WORKSITE_STATE'] = df['WorkState_1']\n",
    "df.drop(columns=['WorkState_1'],inplace=True)\n",
    "\n",
    "\n",
    "df['EMPLOYER_COUNTRY'] = np.nan\n",
    "df['EMPLOYER_PROVINCE'] = np.nan\n",
    "df['EMPLOYER_PHONE'] = np.nan\n",
    "df['EMPLOYER_PHONE_EXT'] = np.nan\n",
    "df['AGENT_REPRESENTING_EMPLOYER'] = np.nan\n",
    "df['AGENT_ATTORNEY_NAME'] = np.nan\n",
    "df['AGENT_ATTORNEY_CITY'] = np.nan\n",
    "df['AGENT_ATTORNEY_STATE'] = np.nan\n",
    "df['H1B_DEPENDENT'] = np.nan\n",
    "df['WILLFUL_VIOLATOR'] = np.nan\n",
    "df['WORKSITE_COUNTY'] = np.nan\n",
    "df['WORKSITE_POSTAL_CODE'] = np.nan\n",
    "df['NAICS_CODE'] = np.nan\n",
    "\n",
    "df_2001_2 = df[MASTER_SCHEMA]\n",
    "if (df_2001_2.columns.tolist() == MASTER_SCHEMA):\n",
    "    df_2001_2.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2001_2.snappy.parquet\"),\n",
    "                       compression='snappy', engine=\"pyarrow\",index=False)\n",
    "    print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2001_2.snappy.parquet\"))\n",
    "print(\"Reading {0} into dataframe\".format(PARQUET_MASTER_SCHEMA+\"/2001_2.snappy.parquet\"))\n",
    "df_new = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2001_2.snappy.parquet\"))\n",
    "\n",
    "# Merge previous and current\n",
    "# df_2016_to_2018 = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2016to2018.snappy.parquet\"))\n",
    "# df_2015_to_2018 = pd.concat([df_2016_to_2018,df_2015])\n",
    "# df_2015_to_2018.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2015to2018.snappy.parquet\"),\n",
    "#                        compression='snappy', engine=\"pyarrow\",index=False)\n",
    "# print(\"Succesfully created: {0}\".format(PARQUET_MASTER_SCHEMA+\"/2015to2018.snappy.parquet\"))\n",
    "# print(\"Current dataset #:{0}, cumulative dataset #: {1}\".format(df_2015.shape,df_2015_to_2018.shape))\n",
    "\n",
    "del df, df_new , df_2001_2 # df_2016_to_2018, df_2015_to_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146684\n",
      "654341\n",
      "624594\n",
      "405641\n",
      "87973\n",
      "288392\n",
      "268243\n",
      "647837\n",
      "9933\n",
      "31489\n",
      "384071\n",
      "442160\n",
      "519476\n",
      "47349\n",
      "415705\n",
      "358817\n",
      "1757\n",
      "307779\n",
      "618781\n",
      "123060\n",
      "31489\n",
      "221262\n",
      "342511\n",
      "308710\n"
     ]
    }
   ],
   "source": [
    "for df in data_frames:\n",
    "    possible_employer_names = ['EmpName', 'EMPLOYER_NAME', 'NAME', 'LCA_CASE_EMPLOYER_NAME']\n",
    "    for i in df.columns.tolist():\n",
    "        if i in possible_employer_names:\n",
    "            print(df[i].count())\n",
    "#             print(df.groupby(i).agg(COUNT_H1Bs=(i,'count')).sort_values('COUNT_H1Bs', ascending=False).head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
