{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FOLDER_NAME = \"LCAProgramsH1BH1B1E3\"\n",
    "# YEAR_MONTH = datetime.today().strftime(\"%Y-%m\")\n",
    "YEAR_MONTH = \"2019-10\"\n",
    "# Assuming all data is inside us-visa-data folder\n",
    "DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR', '../us-visa-data')\n",
    "PARQUET_DIR = DOWNLOAD_DIR + \"/\" + FOLDER_NAME + \"/\" + YEAR_MONTH + \"/downloads/parquet\"\n",
    "PARQUET_MASTER_SCHEMA = PARQUET_DIR + \"/master_schema\"\n",
    "\n",
    "MASTER_SCHEMA = ['YEAR_CASE_BELONGS_TO','CASE_NUMBER','CASE_STATUS','CASE_SUBMITTED','DECISION_DATE','VISA_CLASS','EMPLOYMENT_START_DATE','EMPLOYMENT_END_DATE','EMPLOYER_NAME','EMPLOYER_ADDRESS','EMPLOYER_CITY','EMPLOYER_STATE','EMPLOYER_POSTAL_CODE','EMPLOYER_COUNTRY','EMPLOYER_PROVINCE','EMPLOYER_PHONE','EMPLOYER_PHONE_EXT','AGENT_REPRESENTING_EMPLOYER','AGENT_ATTORNEY_NAME','AGENT_ATTORNEY_CITY','AGENT_ATTORNEY_STATE','JOB_TITLE','SOC_CODE','SOC_NAME','NAICS_CODE','TOTAL_WORKERS','FULL_TIME_POSITION','PREVAILING_WAGE','PW_UNIT_OF_PAY','WAGE_RATE_OF_PAY_FROM','WAGE_RATE_OF_PAY_TO','WAGE_UNIT_OF_PAY_FROM','WAGE_UNIT_OF_PAY_TO','H1B_DEPENDENT','WILLFUL_VIOLATOR','WORKSITE_CITY','WORKSITE_COUNTY','WORKSITE_STATE','WORKSITE_POSTAL_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "files = [f for f in os.listdir(PARQUET_MASTER_SCHEMA) if not f.startswith('.')]\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,file))\n",
    "    dataframes.append(df)\n",
    "    del df\n",
    "df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVAILING_WAGE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were . in the PREVAILING_WAGE column data, replace them\n",
    "# We dint use replace because we need floating values to remain, so replace only exact matches\n",
    "df.loc[df['PREVAILING_WAGE'] == \".\", 'PREVAILING_WAGE'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements have 20-70, in which case average the value and replace\n",
    "import re\n",
    "import statistics\n",
    "import pandas as pd\n",
    "def average_if_hyphen(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        y = re.search('-',str(x))\n",
    "        if y: # if found \n",
    "            return statistics.mean([int(l) for l in str(x).split(\"-\")])\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE'].apply(average_if_hyphen,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5913308, 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements have $ string, remove that\n",
    "import re\n",
    "def replace_dollar(x):\n",
    "    if \"$\" in str(x):\n",
    "        return str(x).replace('$','')\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df['PREVAILING_WAGE'] = df['PREVAILING_WAGE'].apply(replace_dollar,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PREVAILING_WAGE'] = pd.to_numeric(df['PREVAILING_WAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.to_numeric(df['PREVAILING_WAGE']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CASE_SUBMITTED'] = pd.to_datetime(df['CASE_SUBMITTED'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION_DATE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: ('Unknown string format:', '6/1/2006 2')\n",
    "import re\n",
    "def remove_string_after_date(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        y = re.search(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\\s\\d\", str(x))\n",
    "        if y:\n",
    "            return str(x).split(\" \")[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "df['DECISION_DATE'] = df['DECISION_DATE'].apply(remove_string_after_date,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DECISION_DATE'] = pd.to_datetime(df['DECISION_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE_STATUS column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CASE_STATUS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_values = ['Certified','Denied','Pending','PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED',\n",
    "                'Hold','Debarred','roye@fragomen.com','omboko@jacksonlewis.com','aespiritusanto@fragomen.c','mkwok@mltsf.com']\n",
    "replace_with_values = ['CERTIFIED','DENIED','PENDING','PENDING','HOLD','DEBARRED','','','','']\n",
    "df['CASE_STATUS'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CASE_STATUS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CASE_STATUS'] = df['CASE_STATUS'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISA_CLASS column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VISA_CLASS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See H-1B_Record_Layout_FY08.doc\n",
    "import numpy as np\n",
    "match_values = ['R','A','C','S','Select Visa Classification']\n",
    "replace_with_values = ['H-1B','E-3 Australian','H-1B1 Chile','H-1B1 Singapore',np.nan]\n",
    "df['VISA_CLASS'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VISA_CLASS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VISA_CLASS'] = df['VISA_CLASS'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPLOYMENT_START_DATE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2901-02-04 00:00:00\n",
    "def replace_wrong_year(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        if \"2901\" in str(x):\n",
    "            return str(x).replace(\"2901\",\"2019\")\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['EMPLOYMENT_START_DATE'].apply(replace_wrong_year,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError : 70/1, df[df['EMPLOYMENT_START_DATE'].astype(str).str.contains('70')]\n",
    "import re\n",
    "def replace_date_70(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        y = re.search(r\"70/[\\d]{1,2}/[\\d]{4}\",str(x))\n",
    "        if y:\n",
    "            return str(x).replace(\"70\",\"07\")\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['EMPLOYMENT_START_DATE'].apply(replace_date_70,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['EMPLOYMENT_START_DATE'].astype(str).str.contains('70')] - Observe that there are 7001 years.\n",
    "# If you observe the YEAR_CASE_BELONGS_TO, it is clear typo to type 2001\n",
    "import re\n",
    "def replace_date_7001(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        if \"7001\" in str(x):\n",
    "            return str(x).replace(\"7001\",\"2001\")\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "df['EMPLOYMENT_START_DATE'] = df['EMPLOYMENT_START_DATE'].apply(replace_date_7001,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date contained '05/19/1270' - replace with 2001, which is YEAR_CASE_BELONGS_TO\n",
    "df.loc[df['EMPLOYMENT_START_DATE'] == \"05/19/1270\", 'EMPLOYMENT_START_DATE'] = '05/19/2001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date contained '03/70/2001' - replace with 2001, which is YEAR_CASE_BELONGS_TO\n",
    "df.loc[df['EMPLOYMENT_START_DATE'] == \"03/70/2001\", 'EMPLOYMENT_START_DATE'] = '03/07/2001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date contained '??/?7/770?' - replace with nan\n",
    "import numpy as np\n",
    "df.loc[df['EMPLOYMENT_START_DATE'] == \"??/?7/770?\", 'EMPLOYMENT_START_DATE'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\n",
    "# coerce is used as I got tired of fixing data quality issues, for now replacing with NaT (not a time)\n",
    "df['EMPLOYMENT_START_DATE'] = pd.to_datetime(df['EMPLOYMENT_START_DATE'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPLOYMENT_END_DATE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\n",
    "# coerce is used as I got tired of fixing data quality issues, for now replacing with NaT (not a time)\n",
    "df['EMPLOYMENT_END_DATE'] = pd.to_datetime(df['EMPLOYMENT_END_DATE'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOC_CODE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SOC_CODE'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SOC_CODE'] = pd.to_numeric(df['SOC_CODE'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAICS_CODE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NAICS_CODE'].unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOTAL_WORKERS column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TOTAL_WORKERS'].isnull().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TOTAL_WORKERS'] = pd.to_numeric(df['TOTAL_WORKERS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PW_UNIT_OF_PAY column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['PW_UNIT_OF_PAY'].unique()\n",
    "df['PW_UNIT_OF_PAY'] = df['PW_UNIT_OF_PAY'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAGE_RATE_OF_PAY_FROM column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were . in the WAGE_RATE_OF_PAY_FROM column data, replace them\n",
    "# We dint use replace because we need floating values to remain, so replace only exact matches\n",
    "df.loc[df['WAGE_RATE_OF_PAY_FROM'] == \".\", 'WAGE_RATE_OF_PAY_FROM'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: could not convert string to float: '3800\\t.00'\n",
    "# ValueError: could not convert string to float: '42~000.00'\n",
    "df.loc[df['WAGE_RATE_OF_PAY_FROM'] == \"3800\\t.00\", 'WAGE_RATE_OF_PAY_FROM'] = '3800.00'\n",
    "df.loc[df['WAGE_RATE_OF_PAY_FROM'] == \"42~000.00\", 'WAGE_RATE_OF_PAY_FROM'] = '42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: could not convert string to float: '1110???.??' . Got tired at this point, used coerce\n",
    "df['WAGE_RATE_OF_PAY_FROM'] = pd.to_numeric(df['WAGE_RATE_OF_PAY_FROM'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAGE_RATE_OF_PAY_TO column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WAGE_RATE_OF_PAY_TO'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WAGE_RATE_OF_PAY_TO'] = pd.to_numeric(df['WAGE_RATE_OF_PAY_TO'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAGE_UNIT_OF_PAY_FROM column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flcdatacenter.com/CaseH1B.aspx\n",
    "import numpy as np\n",
    "match_values = ['Year','Hour','Month','Week','2 weeks','Select Pay Range']\n",
    "replace_with_values = ['Y','H','M','W','B',np.nan]\n",
    "df['WAGE_UNIT_OF_PAY_FROM'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flcdatacenter.com/CaseH1B.aspx\n",
    "import numpy as np\n",
    "match_values = ['YR','HR','MTH','WK','BI','Bi-Weekly','82,817.00','75,045.00','138,000.00','54,000.00','64,000.00']\n",
    "replace_with_values = ['Y','H','M','W','B','B','Y','Y','Y','Y','Y']\n",
    "df['WAGE_UNIT_OF_PAY_FROM'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flcdatacenter.com/CaseH1B.aspx\n",
    "# How did we know YM = Y ? well print a row that contains YM, observe and make a judgement call\n",
    "import numpy as np\n",
    "match_values = ['YM','MH','YBWH','YH','y','YMBWH','YMBW','WH','yr','hr','mth','wk','bi',None]\n",
    "replace_with_values = ['Y','H','H','Y','Y','Y','H','H','Y','H','M','W','B',np.nan]\n",
    "df['WAGE_UNIT_OF_PAY_FROM'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WAGE_UNIT_OF_PAY_FROM'] = df['WAGE_UNIT_OF_PAY_FROM'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAGE_UNIT_OF_PAY_TO column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WAGE_UNIT_OF_PAY_TO'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flcdatacenter.com/CaseH1B.aspx\n",
    "import numpy as np\n",
    "match_values = ['Year','Hour','Month','Week','2 weeks','Select Pay Range']\n",
    "replace_with_values = ['Y','H','M','W','B',np.nan]\n",
    "df['WAGE_UNIT_OF_PAY_TO'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flcdatacenter.com/CaseH1B.aspx\n",
    "import numpy as np\n",
    "match_values = ['YR','HR','MTH','WK','BI','Bi-Weekly','82,817.00','75,045.00','138,000.00','54,000.00','64,000.00']\n",
    "replace_with_values = ['Y','H','M','W','B','B','Y','Y','Y','Y','Y']\n",
    "df['WAGE_UNIT_OF_PAY_TO'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flcdatacenter.com/CaseH1B.aspx\n",
    "# How did we know YM = Y ? well print a row that contains YM, observe and make a judgement call\n",
    "import numpy as np\n",
    "match_values = ['YM','MH','YBWH','YH','y','YMBWH','YMBW','WH','yr','hr','mth','wk','bi',None]\n",
    "replace_with_values = ['Y','H','H','Y','Y','Y','H','H','Y','H','M','W','B',np.nan]\n",
    "df['WAGE_UNIT_OF_PAY_TO'].replace(to_replace=match_values,value=replace_with_values,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WAGE_UNIT_OF_PAY_TO'] = df['WAGE_UNIT_OF_PAY_TO'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1B_DEPENDENT column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['H1B_DEPENDENT'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKSITE_POSTAL_CODE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WORKSITE_POSTAL_CODE'] = pd.to_numeric(df['WORKSITE_POSTAL_CODE'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPLOYER_POSTAL_CODE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMPLOYER_POSTAL_CODE'] = pd.to_numeric(df['EMPLOYER_POSTAL_CODE'],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to parquet and save back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA, \"2001-2019\" \n",
    "                           + \".snappy.parquet\"), compression='snappy', engine=\"pyarrow\",index=False) # index=False so that row indexes are not saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all salaries to normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FOLDER_NAME = \"LCAProgramsH1BH1B1E3\"\n",
    "# YEAR_MONTH = datetime.today().strftime(\"%Y-%m\")\n",
    "YEAR_MONTH = \"2019-10\"\n",
    "# Assuming all data is inside us-visa-data folder\n",
    "DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR', '../us-visa-data')\n",
    "PARQUET_DIR = DOWNLOAD_DIR + \"/\" + FOLDER_NAME + \"/\" + YEAR_MONTH + \"/downloads/parquet\"\n",
    "PARQUET_MASTER_SCHEMA = PARQUET_DIR + \"/master_schema\"\n",
    "\n",
    "MASTER_SCHEMA = ['YEAR_CASE_BELONGS_TO','CASE_NUMBER','CASE_STATUS','CASE_SUBMITTED','DECISION_DATE','VISA_CLASS','EMPLOYMENT_START_DATE','EMPLOYMENT_END_DATE','EMPLOYER_NAME','EMPLOYER_ADDRESS','EMPLOYER_CITY','EMPLOYER_STATE','EMPLOYER_POSTAL_CODE','EMPLOYER_COUNTRY','EMPLOYER_PROVINCE','EMPLOYER_PHONE','EMPLOYER_PHONE_EXT','AGENT_REPRESENTING_EMPLOYER','AGENT_ATTORNEY_NAME','AGENT_ATTORNEY_CITY','AGENT_ATTORNEY_STATE','JOB_TITLE','SOC_CODE','SOC_NAME','NAICS_CODE','TOTAL_WORKERS','FULL_TIME_POSITION','PREVAILING_WAGE','PW_UNIT_OF_PAY','WAGE_RATE_OF_PAY_FROM','WAGE_RATE_OF_PAY_TO','WAGE_UNIT_OF_PAY_FROM','WAGE_UNIT_OF_PAY_TO','H1B_DEPENDENT','WILLFUL_VIOLATOR','WORKSITE_CITY','WORKSITE_COUNTY','WORKSITE_STATE','WORKSITE_POSTAL_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2001-2019.snappy.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell execution takes more than 15 min\n",
    "# df[(df['YEAR_CASE_BELONGS_TO']==2010)].nlargest(20,['NORMALIZED_SALARY'], keep=\"all\").filter(items=['YEAR_CASE_BELONGS_TO','EMPLOYER_NAME','EMPLOYER_CITY','EMPLOYER_STATE','PW_UNIT_OF_PAY','PREVAILING_WAGE','NORMALIZED_SALARY'])\n",
    "def normalized_salary(row):\n",
    "    if pd.isnull(row['PW_UNIT_OF_PAY']):\n",
    "        return row['PREVAILING_WAGE']\n",
    "    if row['PW_UNIT_OF_PAY'] == 'Year' :\n",
    "        return row['PREVAILING_WAGE']\n",
    "    if row['PW_UNIT_OF_PAY'] == 'Hour':\n",
    "        yearly = row['PREVAILING_WAGE']*8*5*4*12\n",
    "        if yearly < 600000: # making an assumption that no h1b gets paid more than 60000. Also 2010 data has wrongly typed unit of pay...BAD DATA\n",
    "            return yearly\n",
    "        else:\n",
    "            return row['PREVAILING_WAGE'] # assuming that PW_UNIT_OF_PAY has wrong value\n",
    "    if row['PW_UNIT_OF_PAY'] == 'Week' :\n",
    "        return row['PREVAILING_WAGE']*52\n",
    "    if row['PW_UNIT_OF_PAY'] == 'Bi-Weekly':\n",
    "        return row['PREVAILING_WAGE']*24\n",
    "    if row['PW_UNIT_OF_PAY'] == 'Month':\n",
    "        return row['PREVAILING_WAGE']*12    \n",
    "\n",
    "df['NORMALIZED_SALARY'] = df.apply(lambda row: normalized_salary(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA, \"2001-2019_normalized_salary\" \n",
    "                           + \".snappy.parquet\"), compression='snappy', engine=\"pyarrow\",index=False) # index=False so that row indexes are not saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive State from WORKSITE_POSTAL_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install uszipcode==0.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uszipcode import SearchEngine\n",
    "search = SearchEngine(simple_zipcode=True)\n",
    "\n",
    "def return_state(myzip):\n",
    "    if pd.isnull(myzip):\n",
    "        return myzip\n",
    "    else:\n",
    "        zipcode = search.by_zipcode(str(int(myzip)))\n",
    "        zipcode_dict = zipcode.to_dict()\n",
    "        state = zipcode_dict['state']\n",
    "        print(\"State for {0} is {1}\".format(myzip,state))\n",
    "        return state\n",
    "    \n",
    "df['POSTAL_TO_STATE'] = df['WORKSITE_POSTAL_CODE'].apply(return_state,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA, \"2001-2019_normalized_salary_postal_to_state\" \n",
    "                           + \".snappy.parquet\"), compression='snappy', engine=\"pyarrow\",index=False) # index=False so that row indexes are not saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive County (and its FIPS) from WORKSITE_POSTAL_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019_cali = df[(df['POSTAL_TO_STATE']=='CA')&(df['YEAR_CASE_BELONGS_TO']==2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fips = pd.read_csv('fips_county_state.csv',dtype={'FIPS':str,'Name':str,'State':str})\n",
    "df_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uszipcode import SearchEngine\n",
    "search = SearchEngine(simple_zipcode=False)\n",
    "\n",
    "def return_county(myzip):\n",
    "    if pd.isnull(myzip):\n",
    "        return myzip\n",
    "    else:\n",
    "        zipcode = search.by_zipcode(str(int(myzip)))\n",
    "        zipcode_dict = zipcode.to_dict()\n",
    "        county = zipcode_dict['county']\n",
    "        print(\"County for {0} is {1}\".format(myzip,county))\n",
    "        return county\n",
    "    \n",
    "df_2019_cali['POSTAL_TO_COUNTY'] = df_2019_cali['WORKSITE_POSTAL_CODE'].apply(return_county,convert_dtype=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk, because the above cell operation takes more than 30 min to execute\n",
    "df_2019_cali.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA, \"2019_CALI_WORKSITE_POSTAL_TO_COUNTY\" \n",
    "                           + \".snappy.parquet\"), compression='snappy', engine=\"pyarrow\",index=False) # index=False so that row indexes are not saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FOLDER_NAME = \"LCAProgramsH1BH1B1E3\"\n",
    "# YEAR_MONTH = datetime.today().strftime(\"%Y-%m\")\n",
    "YEAR_MONTH = \"2019-10\"\n",
    "# Assuming all data is inside us-visa-data folder\n",
    "DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR', '../../us-visa-data')\n",
    "PARQUET_DIR = DOWNLOAD_DIR + \"/\" + FOLDER_NAME + \"/\" + YEAR_MONTH + \"/downloads/parquet\"\n",
    "PARQUET_MASTER_SCHEMA = PARQUET_DIR + \"/master_schema\"\n",
    "\n",
    "import pandas as pd\n",
    "df_2019_cali = pd.read_parquet(os.path.join(PARQUET_MASTER_SCHEMA,\"2019_CALI_WORKSITE_POSTAL_TO_COUNTY.snappy.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fips(county):\n",
    "    if pd.isnull(county):\n",
    "        return county\n",
    "    else:\n",
    "        fips = df_fips[df_fips['Name'] == county.replace('County','').strip()]['FIPS'].values[0]\n",
    "        print(\"Fips for {0} is {1}\".format(county,fips))\n",
    "        return fips\n",
    "    \n",
    "df_2019_cali['FIPS'] = df_2019_cali['POSTAL_TO_COUNTY'].apply(return_fips,convert_dtype=False).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk, because the above cell operation takes more than 30 min to execute\n",
    "df_2019_cali.to_parquet(os.path.join(PARQUET_MASTER_SCHEMA, \"2019_CALI_WORKSITE_POSTAL_TO_COUNTY_FIPS\" \n",
    "                           + \".snappy.parquet\"), compression='snappy', engine=\"pyarrow\",index=False) # index=False so that row indexes are not saved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
